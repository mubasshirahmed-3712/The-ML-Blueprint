# ğŸ”¹ Model Tuning in Machine Learning (Classification Focus)

When we train classification models (Logistic Regression, KNN, SVM, Random Forest, etc.), their performance depends on three things:  

1. **How we split & evaluate data** (Train/Test, Cross Validation)  
2. **Which hyperparameters we choose** (C in SVM, k in KNN, max depth in trees)  
3. **Which metrics we optimize** (Accuracy? F1? AUC?)  

ğŸ‘‰ **Model Tuning** = systematically improving these three steps.  
This folder demonstrates model tuning using **Support Vector Machine (SVM)** on the `Social_Network_Ads.csv` dataset.  

---

## ğŸ“‚ Folder Structure

```
8.Model_Tuning/
â”œâ”€ data/
â”‚  â””â”€ Social_Network_Ads.csv
â””â”€ notebooks/
   â”œâ”€ 1_SVM_with_CrossValidation.ipynb
   â”œâ”€ 2_SVM_with_GridSearchCV.ipynb
   â”œâ”€ 3_SVM_with_RandomSearchCV.ipynb
   â”œâ”€ 4_SVM_with_ROC_AUC.ipynb
   â””â”€ 5_Final_Summary.ipynb
```

---

## ğŸ”¹ 1. Cross Validation (CV)

### â“ Problem:
A simple train-test split is unstable. Accuracy depends on random split â†’ model may look better/worse than it actually is.  

### âœ… Solution: **K-Fold Cross Validation**  
- Split dataset into **k folds** (e.g., k=5).  
- Train on k-1 folds, test on remaining fold.  
- Repeat for all folds.  
- Take average performance â†’ more reliable.  

âš¡ **Syntax**:

```python
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5)
print("Mean Accuracy:", accuracies.mean())
print("Std Deviation:", accuracies.std())
```

---

## ğŸ”¹ 2. Grid Search

### â“ Problem:
Algorithms have hyperparameters (not learned automatically). Example:  
- SVM â†’ `C`, `kernel`, `gamma`  
- KNN â†’ number of neighbors `k`  
- Decision Trees â†’ `max_depth`, `min_samples_split`  

Choosing manually = guesswork.  

### âœ… Solution: **GridSearchCV**  
- Define a **grid (dictionary)** of possible hyperparameter values.  
- Train models for **all combinations**.  
- Evaluate with CV.  
- Select the **best parameters**.  

âš¡ **Syntax**:

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    "C": [0.1, 1, 10, 100],
    "kernel": ["linear", "rbf"],
    "gamma": [0.1, 0.5, 1.0]
}

grid = GridSearchCV(SVC(), param_grid, cv=10, scoring="accuracy")
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
print("Best Accuracy:", grid.best_score_)
```

---

## ğŸ”¹ 3. Random Search

### â“ Problem:
GridSearch tests *all* combinations â†’ slow when search space is large.  

### âœ… Solution: **RandomizedSearchCV**  
- Randomly sample combinations for fixed number of iterations (`n_iter`).  
- Much faster.  
- Often finds near-optimal solution.  

âš¡ **Syntax**:

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform

param_dist = {
    "C": [0.1, 1, 10, 100, 1000],
    "kernel": ["linear", "rbf"],
    "gamma": uniform(0.1, 1.0)
}

random_search = RandomizedSearchCV(
    estimator=SVC(),
    param_distributions=param_dist,
    n_iter=20, cv=10, scoring="accuracy", random_state=0
)
random_search.fit(X_train, y_train)

print("Best Parameters:", random_search.best_params_)
print("Best Accuracy:", random_search.best_score_)
```

---

## ğŸ”¹ 4. Performance Metrics Beyond Accuracy

Accuracy alone can be misleading (e.g., in imbalanced data).  

### ğŸ“Œ Confusion Matrix  

|               | Predicted Positive | Predicted Negative |
|---------------|-------------------|-------------------|
| **Actual Pos** | True Positive (TP) | False Negative (FN) |
| **Actual Neg** | False Positive (FP) | True Negative (TN) |

---

### ğŸ“Œ Precision & Recall  

- **Precision** = TP / (TP + FP)  
ğŸ‘‰ Out of predicted positives, how many are correct?  

- **Recall (Sensitivity/TPR)** = TP / (TP + FN)  
ğŸ‘‰ Out of actual positives, how many did we catch?  

---

### ğŸ“Œ F1 Score  

- Harmonic mean of Precision & Recall.  
- Good for imbalanced datasets.  

```python
from sklearn.metrics import precision_score, recall_score, f1_score
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
```

---

### ğŸ“Œ ROC & AUC  

- **ROC Curve** = plots TPR (Recall) vs FPR at different thresholds.  
- **AUC** = probability model ranks a random positive higher than random negative.  
  - Perfect model â†’ AUC = 1.0  
  - Random guessing â†’ AUC = 0.5  

âš¡ **Syntax**:

```python
from sklearn.metrics import roc_curve, roc_auc_score
y_prob = classifier.predict_proba(X_test)[:,1]

fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)

print("ROC AUC:", auc)
```

ğŸ“Š Example ROC Curve:  

![ROC Curve Example](https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_001.png)  

---

## ğŸ”¹ 5. Practical Tuning Workflow

1. Split data â†’ Train/Test  
2. Use **K-Fold CV** on training data  
3. Use **GridSearchCV or RandomizedSearchCV** for hyperparameter tuning  
4. Evaluate final tuned model on Test Set  
5. Report metrics â†’ Confusion Matrix, Precision, Recall, F1, ROCâ€“AUC  

---

## âœ… Final Takeaways

- **Cross Validation** â†’ reliable performance estimate  
- **Grid Search** â†’ systematic tuning (but slow)  
- **Random Search** â†’ faster, near-optimal  
- **Metrics** â†’ use Precision, Recall, F1, ROCâ€“AUC (not just Accuracy)  
- **SVM Tuning Example** shows how all these fit together  

ğŸ‘‰ Without tuning & proper metrics, you risk deploying a model that looks good on a test split but fails in real-world data.  

---

ğŸ“Œ This README is not just project documentation â€” itâ€™s also a **study guide** for anyone learning model tuning in classification.  
