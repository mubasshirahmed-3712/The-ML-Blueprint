{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# <h2 style=\"text-align:center;\">XGBoost Classification</h2>\n",
    "# ==========================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≥ Introduction to XGBoost\n",
    "\n",
    "**XGBoost (Extreme Gradient Boosting)** is one of the most powerful and widely used ensemble learning algorithms, especially in **classification and regression tasks**.\n",
    "\n",
    "It is based on the principle of **Boosting**, which is an ensemble technique that combines multiple weak learners (usually decision trees) to create a strong learner.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Key Concepts of XGBoost\n",
    "\n",
    "1. **Boosting**  \n",
    "   - Unlike Bagging (Random Forest), which trains models independently, Boosting trains models sequentially.  \n",
    "   - Each new tree focuses on correcting the errors of the previous ones.  \n",
    "\n",
    "2. **Gradient Boosting**  \n",
    "   - Uses gradient descent to minimize errors.  \n",
    "   - XGBoost is an optimized version of Gradient Boosting with additional techniques like **regularization** and **parallel processing**.\n",
    "\n",
    "3. **Advantages of XGBoost**  \n",
    "   - High performance & accuracy  \n",
    "   - Handles missing values well  \n",
    "   - Built-in regularization (to reduce overfitting)  \n",
    "   - Works well with structured/tabular data  \n",
    "   - Faster training using parallel computation  \n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Where is XGBoost used?\n",
    "- Credit risk modeling  \n",
    "- Customer churn prediction  \n",
    "- Fraud detection  \n",
    "- Kaggle competitions (very popular)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Importing the libraries\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (10000, 10)\n",
      "Shape of y: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Importing the dataset\n",
    "# ==========================================================\n",
    "dataset = pd.read_csv(\"../data/Churn_Modelling.csv\")\n",
    "\n",
    "# Independent features (excluding RowNumber, CustomerId, Surname, and last column)\n",
    "X = dataset.iloc[:, 3:-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "print(\"Shape of X:\", X.shape)  # ‚û§ Check dimensions\n",
    "print(\"Shape of y:\", y.shape)  # ‚û§ Target vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Encoding Shape: (10000, 12)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Encoding categorical data\n",
    "# ==========================================================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Encode \"Gender\" column\n",
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:, 2])\n",
    "\n",
    "# OneHotEncode \"Geography\" column\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('encoder', OneHotEncoder(), [1])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "print(\"After Encoding Shape:\", X.shape)  # ‚û§ Check new shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (8000, 12)\n",
      "Test set shape: (2000, 12)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Splitting the dataset into Training & Test sets\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost Model Trained Successfully\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Training the XGBoost Classifier\n",
    "# ==========================================================\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "classifier = XGBClassifier(random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ XGBoost Model Trained Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions: [1 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Predicting the Test set results\n",
    "# ==========================================================\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Sample Predictions:\", y_pred[:10])  # ‚û§ Show first 10 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1496   99]\n",
      " [ 192  213]]\n",
      "Accuracy Score: 0.8545\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Model Evaluation (Confusion Matrix & Accuracy)\n",
    "# ==========================================================\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy Score:\", ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias (Train Accuracy): 0.953875\n",
      "Variance (Test Accuracy): 0.8545\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Bias-Variance Analysis\n",
    "# ==========================================================\n",
    "bias = classifier.score(X_train, y_train)   # Training accuracy\n",
    "variance = classifier.score(X_test, y_test) # Testing accuracy\n",
    "\n",
    "print(\"Bias (Train Accuracy):\", bias)\n",
    "print(\"Variance (Test Accuracy):\", variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Mean: 85.15 %\n",
      "Cross-Validation Std Dev: 0.48 %\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Applying k-Fold Cross Validation\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5)\n",
    "print(\"Cross-Validation Accuracy Mean: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Cross-Validation Std Dev: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Summary of XGBoost Classification\n",
    "\n",
    "In this notebook, we performed **customer churn prediction** using **XGBoost Classifier**.\n",
    "\n",
    "### Steps we covered:\n",
    "1. Imported & preprocessed the dataset  \n",
    "   - Encoded categorical features (Geography & Gender)  \n",
    "   - One-hot encoding applied  \n",
    "2. Split data into training and testing sets  \n",
    "3. Trained an **XGBoost classifier**  \n",
    "4. Evaluated the model with  \n",
    "   - Confusion Matrix  \n",
    "   - Accuracy Score  \n",
    "   - Bias-Variance check  \n",
    "   - k-Fold Cross Validation  \n",
    "\n",
    "### üîë Key Takeaways\n",
    "- XGBoost is a **Boosting-based Ensemble Learning algorithm**.  \n",
    "- It reduces **bias and variance** effectively compared to a single Decision Tree.  \n",
    "- Provides **high accuracy** and is widely used in real-world ML problems.  \n",
    "- Works very well for **structured data** like customer churn, credit scoring, etc.  \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNExMzVFPH/QpXk+BsMtqvF",
   "collapsed_sections": [],
   "name": "XGBoost",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
