
# ğŸ¤– Clustering Algorithms in Machine Learning

> *"Clustering is the art of discovering structure in data without labels."*  
This module explores **unsupervised learning techniques** where the goal is to group similar data points into clusters. Unlike supervised models, clustering does not rely on predefined outputs but instead uncovers hidden patterns.

---

## ğŸ“‚ Folder Structure
```
3.Clustering/
â”‚â”€â”€ 1.KMeans/
â”‚   â”œâ”€â”€ concepts/              # Visual explanations of KMeans steps
â”‚   â”œâ”€â”€ data/                  # Mall_Customers dataset
â”‚   â””â”€â”€ notebooks/             # Jupyter notebooks (basic + formatted)
â”‚
â”‚â”€â”€ 2.Hierarchical/
â”‚   â”œâ”€â”€ concepts/              # Dendrogram and hierarchy visuals
â”‚   â”œâ”€â”€ data/                  # Mall_Customers dataset
â”‚   â””â”€â”€ notebooks/             # Jupyter notebooks
â”‚
â”‚â”€â”€ 3.DBScan/
â”‚   â”œâ”€â”€ concepts/              # Density-based clustering visuals
â”‚   â””â”€â”€ notebooks/             # Jupyter notebooks
```

---

## ğŸ§­ Introduction to Clustering
Clustering is an **unsupervised learning task** where data points are grouped based on similarity.  
It is widely used in:
- ğŸ“Š Customer segmentation  
- ğŸ§¬ Gene sequence analysis  
- ğŸ›ï¸ Market basket analysis  
- ğŸ“ˆ Anomaly detection  

---

## 1ï¸âƒ£ KMeans Clustering

### ğŸ”¹ Concept
KMeans partitions the dataset into **K distinct clusters** based on the nearest mean (centroid).  

- **Steps:**
  1. Choose the number of clusters (K).  
  2. Initialize centroids randomly.  
  3. Assign points to the nearest centroid.  
  4. Update centroids as mean of points.  
  5. Repeat until convergence.  

- **Math Formula:**  
  Minimize the objective function:  
  J = Î£ (||x - Î¼||Â²) over all clusters

### ğŸ”¹ In This Folder
- **Concepts:** Step-by-step visuals of centroid updates.  
- **Data:** Mall_Customers.csv dataset for clustering customer profiles.  
- **Notebooks:** `KMeans_Formatted.ipynb` explains the workflow clearly.  

### ğŸ”¹ Learning Outcome
- Understand how **centroids move** with iterations.  
- Learn how to choose **optimal K using the Elbow Method & Silhouette Score**.  

---

## 2ï¸âƒ£ Hierarchical Clustering

### ğŸ”¹ Concept
Hierarchical clustering builds a **tree of clusters (dendrogram)**.  
Two main approaches:
- **Agglomerative (bottom-up)**: Merge clusters iteratively.  
- **Divisive (top-down)**: Split clusters recursively.  

- **Distance Metrics Used:**
  - Euclidean, Manhattan, Cosine, etc.  

- **Linkage Methods:**
  - Single, Complete, Average, Wardâ€™s Method.  

### ğŸ”¹ In This Folder
- **Concepts:** Visuals of dendrograms and merges.  
- **Data:** Mall_Customers.csv for hierarchical clustering.  
- **Notebooks:** `hierarchy.ipynb` demonstrates Agglomerative clustering.  

### ğŸ”¹ Learning Outcome
- Learn how to **interpret dendrograms**.  
- Understand **linkage differences** and how they affect results.  

---

## 3ï¸âƒ£ DBSCAN (Density-Based Spatial Clustering)

### ğŸ”¹ Concept
DBSCAN groups points that are **closely packed together** and labels points in sparse regions as **outliers/noise**.  

- **Parameters:**
  - `eps`: Maximum distance between two samples for them to be neighbors.  
  - `minPts`: Minimum number of points to form a dense cluster.  

- **Strengths:**  
  - Can find arbitrarily shaped clusters.  
  - Identifies noise/outliers naturally.  

- **Weaknesses:**  
  - Sensitive to parameter choice (`eps`, `minPts`).  

### ğŸ”¹ In This Folder
- **Concepts:** Illustrations of density and clusters.  
- **Notebooks:** `DB_SCAN CLUSTERING ALGORITHM.ipynb`.  

### ğŸ”¹ Learning Outcome
- Understand how density affects cluster formation.  
- Learn to detect anomalies in datasets using DBSCAN.  

---

## ğŸš€ Workflow for Clustering
1. Load dataset (`Mall_Customers.csv`).  
2. Preprocess (scaling if necessary).  
3. Apply algorithm (KMeans, Hierarchical, or DBSCAN).  
4. Visualize clusters in 2D/3D.  
5. Evaluate using metrics (Silhouette Score, Davies-Bouldin Index).  

---

## ğŸ¯ Final Takeaways
- **KMeans** â†’ Fast, scalable, but needs K beforehand.  
- **Hierarchical** â†’ Provides hierarchy & dendrograms but less scalable.  
- **DBSCAN** â†’ Great for irregular clusters & outlier detection.  

ğŸ“Œ This folder equips you with **theory + implementation + visualization** of clustering algorithms in Python.
